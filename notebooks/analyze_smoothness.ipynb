{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "---\n",
    "The purpose of this notebook is to generate and compute the 1-edit distance between all the possible rooted and unlabeled trees given a fixed `depth` and `maximum_degree`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook configuration\n",
    "Configure several notebook configuration settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:98% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Disable some warnings\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "# Display all cell outputs\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'last_expr'\n",
    "\n",
    "# Use full screen width\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:98% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries / Packages\n",
    "Import several useful packages for the notebook and configure some extra options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Miscellaneous libraries\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "# Setup some options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 100\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "matplotlib.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching data to analyze\n",
    "---\n",
    "Fetch all the generated `smoothness` csv files generated and analyze them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mN100_n100_p0.1_1624367054\u001b[m\u001b[m \u001b[1m\u001b[34mN100_n30_p0.1_1624230420\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../data/synthetic/erdos_renyi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373362910\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373363205\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373363387\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373363566\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373363742\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373363934\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373364137\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373364321\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373364499\u001b[m\u001b[m\r\n",
      "\u001b[1m\u001b[34mnum1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1__1624373364675\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "!ls \"../data/synthetic/erdos_renyi/N100_n100_p0.1_1624367054/teacher_outputs/regression/GIN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results over 10 different teacher initializations, using the folowing configuration:\n",
      "num1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1\n",
      "\n",
      "Student outputs computed with the following models: GCN, Baseline, GIN\n",
      "\n",
      "Smoothness computed with the following distance matrices: WL__hashing__d3_iOnes__hamming__sMaxdegree\n"
     ]
    }
   ],
   "source": [
    "teacher_outputs_prefix = \"../data/synthetic/erdos_renyi/N100_n100_p0.1_1624367054/teacher_outputs/regression/GIN\"\n",
    "\n",
    "# Retrieve different teacher output configurations\n",
    "teacher_outputs_filenames = [x for x in os.listdir(teacher_outputs_prefix) if all([c.isdigit() for c in x[-13:]])]\n",
    "teacher_outputs_configs = ['__'.join(x.split('__')[:-1]) for x in teacher_outputs_filenames]\n",
    "# Make sure to work only with the same teacher configurations\n",
    "assert teacher_outputs_configs.count(teacher_outputs_configs[0]) == len(teacher_outputs_configs), \\\n",
    "    'Please specify teacher configuration! (there are more than one configuration available)'\n",
    "print(f\"Results over {len(teacher_outputs_filenames)} different teacher initializations, using the folowing configuration:\")\n",
    "print(teacher_outputs_filenames[0][:-15])\n",
    "\n",
    "# Retrieve available student models\n",
    "students = set([\n",
    "    x for x in os.listdir(f'{teacher_outputs_prefix}/{teacher_outputs_filenames[0]}/student_outputs') if x != '.DS_Store'\n",
    "])\n",
    "print()\n",
    "print(f\"Student outputs computed with the following models: {', '.join(students)}\")\n",
    "\n",
    "# Retrieve the different distance matrices used for computing the smoothness\n",
    "dist_matrices = set()\n",
    "for teacher_outputs_filename in teacher_outputs_filenames:\n",
    "    for student in students:\n",
    "        dist_matrix_smoothness = [\n",
    "            x for x in os.listdir(f'{teacher_outputs_prefix}/{teacher_outputs_filename}/student_outputs/{student}/smoothness') \n",
    "            if x.endswith('.csv')\n",
    "        ]\n",
    "        for dist_matrix in dist_matrix_smoothness:\n",
    "            dist_matrices.add(dist_matrix.rstrip('.csv'))\n",
    "print()\n",
    "print(f\"Smoothness computed with the following distance matrices: {', '.join(dist_matrices)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Fix the `dist matrix` and the `teacher architecture`, and read all the available data for all the available combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIST_MATRIX = 'WL__hashing__d3_iOnes__hamming__sMaxdegree'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some auxiliary functions for reading the generated smoothness csv files\n",
    "def readSmoothnessCSV(smoothness_filename, model):\n",
    "    '''Auxiliary function to read a generated smoothness csv file.'''\n",
    "    smoothness_stats = (pd.read_csv(smoothness_filename, header=[0, 1], index_col=0)\n",
    "                          .reset_index()\n",
    "                          .rename(columns={'index': 'model_config'}))\n",
    "    # Add an extra column with the model type (for later processing)\n",
    "    smoothness_stats['model'] = [model] * len(smoothness_stats)\n",
    "    # Rearange column order for better visualization\n",
    "    cols = smoothness_stats.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    return smoothness_stats[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_config</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0</th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">3</th>\n",
       "      <th colspan=\"4\" halign=\"left\">4</th>\n",
       "      <th colspan=\"4\" halign=\"left\">5</th>\n",
       "      <th colspan=\"4\" halign=\"left\">6</th>\n",
       "      <th colspan=\"4\" halign=\"left\">7</th>\n",
       "      <th colspan=\"4\" halign=\"left\">8</th>\n",
       "      <th colspan=\"4\" halign=\"left\">9</th>\n",
       "      <th colspan=\"4\" halign=\"left\">10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.161347</td>\n",
       "      <td>0.284698</td>\n",
       "      <td>0.517959</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>3.704228</td>\n",
       "      <td>0.448261</td>\n",
       "      <td>0.031732</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>3.931013</td>\n",
       "      <td>0.169515</td>\n",
       "      <td>0.021079</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>4.059537</td>\n",
       "      <td>0.144355</td>\n",
       "      <td>0.008320</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>4.154589</td>\n",
       "      <td>0.073464</td>\n",
       "      <td>0.005507</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>4.172434</td>\n",
       "      <td>0.030223</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>4.163180</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>4.162100</td>\n",
       "      <td>0.021542</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>4.171343</td>\n",
       "      <td>0.019862</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>4.217849</td>\n",
       "      <td>0.069332</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>4.20240</td>\n",
       "      <td>0.075107</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.003426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>1.444393</td>\n",
       "      <td>1.469585</td>\n",
       "      <td>0.525974</td>\n",
       "      <td>0.286055</td>\n",
       "      <td>4.114533</td>\n",
       "      <td>0.092545</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>4.128061</td>\n",
       "      <td>0.116017</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>4.164031</td>\n",
       "      <td>0.050895</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>4.182329</td>\n",
       "      <td>0.024355</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>4.175763</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>4.175362</td>\n",
       "      <td>0.025934</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>4.187974</td>\n",
       "      <td>0.044935</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>4.161577</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>4.171169</td>\n",
       "      <td>0.026598</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>4.17387</td>\n",
       "      <td>0.019045</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model                                       model_config               0  \\\n",
       "                                                           mean_smoothness   \n",
       "0   GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...        0.161347   \n",
       "1   GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...        1.444393   \n",
       "\n",
       "                                                          1  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.284698  0.517959     0.081921        3.704228   \n",
       "1           1.469585  0.525974     0.286055        4.114533   \n",
       "\n",
       "                                                          2  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.448261  0.031732     0.011981        3.931013   \n",
       "1           0.092545  0.008864     0.007002        4.128061   \n",
       "\n",
       "                                                          3  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.169515  0.021079     0.011209        4.059537   \n",
       "1           0.116017  0.004978     0.004651        4.164031   \n",
       "\n",
       "                                                          4  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.144355  0.008320     0.005738        4.154589   \n",
       "1           0.050895  0.002522     0.002398        4.182329   \n",
       "\n",
       "                                                          5  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.073464  0.005507     0.003681        4.172434   \n",
       "1           0.024355  0.002344     0.001521        4.175763   \n",
       "\n",
       "                                                          6  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.030223  0.003746     0.001805        4.163180   \n",
       "1           0.014542  0.001616     0.001149        4.175362   \n",
       "\n",
       "                                                          7  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.019712  0.002774     0.000978        4.162100   \n",
       "1           0.025934  0.002340     0.001987        4.187974   \n",
       "\n",
       "                                                          8  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.021542  0.002881     0.001098        4.171343   \n",
       "1           0.044935  0.002512     0.002272        4.161577   \n",
       "\n",
       "                                                          9  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.019862  0.003001     0.000853        4.217849   \n",
       "1           0.043516  0.002041     0.001230        4.171169   \n",
       "\n",
       "                                                         10  \\\n",
       "  std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0           0.069332  0.005297     0.003443         4.20240   \n",
       "1           0.026598  0.001654     0.001563         4.17387   \n",
       "\n",
       "                                             \n",
       "  std_dev_smoothness mean_rmse std_dev_rmse  \n",
       "0           0.075107  0.003977     0.003426  \n",
       "1           0.019045  0.001518     0.000908  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test method with a sample\n",
    "sample_filename = f\"{teacher_outputs_prefix}/{teacher_outputs_filenames[0]}/student_outputs/GIN/smoothness/{DIST_MATRIX}.csv\"\n",
    "sample_smoothness_stats = readSmoothnessCSV(sample_filename, model='GIN')\n",
    "# Save ordered columns for later processing\n",
    "ordered_cols = sample_smoothness_stats.columns.tolist()\n",
    "sample_smoothness_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fb4665d23a742fdb5dc2f83d0ef729e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>model_config</th>\n",
       "      <th colspan=\"4\" halign=\"left\">0</th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">3</th>\n",
       "      <th colspan=\"4\" halign=\"left\">4</th>\n",
       "      <th colspan=\"4\" halign=\"left\">5</th>\n",
       "      <th colspan=\"4\" halign=\"left\">6</th>\n",
       "      <th colspan=\"4\" halign=\"left\">7</th>\n",
       "      <th colspan=\"4\" halign=\"left\">8</th>\n",
       "      <th colspan=\"4\" halign=\"left\">9</th>\n",
       "      <th colspan=\"4\" halign=\"left\">10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.205909</td>\n",
       "      <td>0.341442</td>\n",
       "      <td>0.531926</td>\n",
       "      <td>0.201731</td>\n",
       "      <td>2.307958</td>\n",
       "      <td>0.786470</td>\n",
       "      <td>0.051742</td>\n",
       "      <td>0.015251</td>\n",
       "      <td>3.633380</td>\n",
       "      <td>0.088388</td>\n",
       "      <td>0.029570</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>3.691302</td>\n",
       "      <td>0.032049</td>\n",
       "      <td>0.027370</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>3.692072</td>\n",
       "      <td>0.039801</td>\n",
       "      <td>0.026823</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>3.724764</td>\n",
       "      <td>0.077982</td>\n",
       "      <td>0.026478</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>3.763451</td>\n",
       "      <td>0.111242</td>\n",
       "      <td>0.027051</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>3.729073</td>\n",
       "      <td>0.080766</td>\n",
       "      <td>0.026336</td>\n",
       "      <td>0.000657</td>\n",
       "      <td>3.738248</td>\n",
       "      <td>0.089054</td>\n",
       "      <td>0.026059</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>3.731122</td>\n",
       "      <td>0.092032</td>\n",
       "      <td>0.026013</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>3.747509</td>\n",
       "      <td>0.068481</td>\n",
       "      <td>0.025937</td>\n",
       "      <td>0.000866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>4.169379</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.161347</td>\n",
       "      <td>0.284698</td>\n",
       "      <td>0.517959</td>\n",
       "      <td>0.081921</td>\n",
       "      <td>3.704228</td>\n",
       "      <td>0.448261</td>\n",
       "      <td>0.031732</td>\n",
       "      <td>0.011981</td>\n",
       "      <td>3.931013</td>\n",
       "      <td>0.169515</td>\n",
       "      <td>0.021079</td>\n",
       "      <td>0.011209</td>\n",
       "      <td>4.059537</td>\n",
       "      <td>0.144355</td>\n",
       "      <td>0.008320</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>4.154589</td>\n",
       "      <td>0.073464</td>\n",
       "      <td>0.005507</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>4.172434</td>\n",
       "      <td>0.030223</td>\n",
       "      <td>0.003746</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>4.163180</td>\n",
       "      <td>0.019712</td>\n",
       "      <td>0.002774</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>4.162100</td>\n",
       "      <td>0.021542</td>\n",
       "      <td>0.002881</td>\n",
       "      <td>0.001098</td>\n",
       "      <td>4.171343</td>\n",
       "      <td>0.019862</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>4.217849</td>\n",
       "      <td>0.069332</td>\n",
       "      <td>0.005297</td>\n",
       "      <td>0.003443</td>\n",
       "      <td>4.202400</td>\n",
       "      <td>0.075107</td>\n",
       "      <td>0.003977</td>\n",
       "      <td>0.003426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>1.444393</td>\n",
       "      <td>1.469585</td>\n",
       "      <td>0.525974</td>\n",
       "      <td>0.286055</td>\n",
       "      <td>4.114533</td>\n",
       "      <td>0.092545</td>\n",
       "      <td>0.008864</td>\n",
       "      <td>0.007002</td>\n",
       "      <td>4.128061</td>\n",
       "      <td>0.116017</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.004651</td>\n",
       "      <td>4.164031</td>\n",
       "      <td>0.050895</td>\n",
       "      <td>0.002522</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>4.182329</td>\n",
       "      <td>0.024355</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>4.175763</td>\n",
       "      <td>0.014542</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>4.175362</td>\n",
       "      <td>0.025934</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>4.187974</td>\n",
       "      <td>0.044935</td>\n",
       "      <td>0.002512</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>4.161577</td>\n",
       "      <td>0.043516</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>4.171169</td>\n",
       "      <td>0.026598</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>4.173870</td>\n",
       "      <td>0.019045</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.000908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.113081</td>\n",
       "      <td>0.079308</td>\n",
       "      <td>0.446170</td>\n",
       "      <td>0.177292</td>\n",
       "      <td>1.692998</td>\n",
       "      <td>0.389816</td>\n",
       "      <td>0.065604</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>2.438547</td>\n",
       "      <td>0.219093</td>\n",
       "      <td>0.043552</td>\n",
       "      <td>0.009089</td>\n",
       "      <td>2.372230</td>\n",
       "      <td>0.206250</td>\n",
       "      <td>0.045270</td>\n",
       "      <td>0.008768</td>\n",
       "      <td>2.347039</td>\n",
       "      <td>0.180686</td>\n",
       "      <td>0.044804</td>\n",
       "      <td>0.007723</td>\n",
       "      <td>2.285376</td>\n",
       "      <td>0.177515</td>\n",
       "      <td>0.046246</td>\n",
       "      <td>0.007707</td>\n",
       "      <td>2.217949</td>\n",
       "      <td>0.179131</td>\n",
       "      <td>0.050430</td>\n",
       "      <td>0.008644</td>\n",
       "      <td>2.253892</td>\n",
       "      <td>0.159414</td>\n",
       "      <td>0.050430</td>\n",
       "      <td>0.008730</td>\n",
       "      <td>2.211590</td>\n",
       "      <td>0.171910</td>\n",
       "      <td>0.051626</td>\n",
       "      <td>0.009626</td>\n",
       "      <td>2.217518</td>\n",
       "      <td>0.164647</td>\n",
       "      <td>0.054593</td>\n",
       "      <td>0.009983</td>\n",
       "      <td>2.153550</td>\n",
       "      <td>0.135963</td>\n",
       "      <td>0.057452</td>\n",
       "      <td>0.010208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>3.285040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016575</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.160057</td>\n",
       "      <td>0.275136</td>\n",
       "      <td>0.531294</td>\n",
       "      <td>0.083877</td>\n",
       "      <td>3.060610</td>\n",
       "      <td>0.333263</td>\n",
       "      <td>0.028846</td>\n",
       "      <td>0.018347</td>\n",
       "      <td>3.194418</td>\n",
       "      <td>0.150212</td>\n",
       "      <td>0.015151</td>\n",
       "      <td>0.008145</td>\n",
       "      <td>3.260792</td>\n",
       "      <td>0.080157</td>\n",
       "      <td>0.008223</td>\n",
       "      <td>0.004532</td>\n",
       "      <td>3.293764</td>\n",
       "      <td>0.056251</td>\n",
       "      <td>0.006305</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>3.333624</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.003667</td>\n",
       "      <td>0.002112</td>\n",
       "      <td>3.293128</td>\n",
       "      <td>0.067839</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>0.003494</td>\n",
       "      <td>3.314878</td>\n",
       "      <td>0.059871</td>\n",
       "      <td>0.004388</td>\n",
       "      <td>0.004084</td>\n",
       "      <td>3.312380</td>\n",
       "      <td>0.041183</td>\n",
       "      <td>0.002994</td>\n",
       "      <td>0.002430</td>\n",
       "      <td>3.350938</td>\n",
       "      <td>0.081388</td>\n",
       "      <td>0.003935</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>3.301237</td>\n",
       "      <td>0.122493</td>\n",
       "      <td>0.006421</td>\n",
       "      <td>0.005248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>1.936377</td>\n",
       "      <td>2.761636</td>\n",
       "      <td>0.406734</td>\n",
       "      <td>0.246671</td>\n",
       "      <td>3.317684</td>\n",
       "      <td>0.033607</td>\n",
       "      <td>0.005737</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>3.311760</td>\n",
       "      <td>0.073731</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.003572</td>\n",
       "      <td>3.231727</td>\n",
       "      <td>0.171514</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.006501</td>\n",
       "      <td>3.332320</td>\n",
       "      <td>0.020489</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.001196</td>\n",
       "      <td>3.330360</td>\n",
       "      <td>0.030402</td>\n",
       "      <td>0.002899</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>3.302647</td>\n",
       "      <td>0.091246</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.007965</td>\n",
       "      <td>3.330710</td>\n",
       "      <td>0.023717</td>\n",
       "      <td>0.001889</td>\n",
       "      <td>0.000999</td>\n",
       "      <td>3.336876</td>\n",
       "      <td>0.021908</td>\n",
       "      <td>0.001816</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>3.279300</td>\n",
       "      <td>0.133599</td>\n",
       "      <td>0.005876</td>\n",
       "      <td>0.009684</td>\n",
       "      <td>3.308046</td>\n",
       "      <td>0.060174</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.005459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.073010</td>\n",
       "      <td>0.067966</td>\n",
       "      <td>0.740877</td>\n",
       "      <td>0.402790</td>\n",
       "      <td>0.113930</td>\n",
       "      <td>0.040076</td>\n",
       "      <td>0.083571</td>\n",
       "      <td>0.026333</td>\n",
       "      <td>0.134291</td>\n",
       "      <td>0.019999</td>\n",
       "      <td>0.073168</td>\n",
       "      <td>0.015674</td>\n",
       "      <td>0.132592</td>\n",
       "      <td>0.019993</td>\n",
       "      <td>0.065472</td>\n",
       "      <td>0.008869</td>\n",
       "      <td>0.132847</td>\n",
       "      <td>0.018081</td>\n",
       "      <td>0.066003</td>\n",
       "      <td>0.009502</td>\n",
       "      <td>0.133005</td>\n",
       "      <td>0.019803</td>\n",
       "      <td>0.067992</td>\n",
       "      <td>0.013056</td>\n",
       "      <td>0.129370</td>\n",
       "      <td>0.021930</td>\n",
       "      <td>0.071525</td>\n",
       "      <td>0.015650</td>\n",
       "      <td>0.130870</td>\n",
       "      <td>0.019559</td>\n",
       "      <td>0.069755</td>\n",
       "      <td>0.016234</td>\n",
       "      <td>0.130774</td>\n",
       "      <td>0.018659</td>\n",
       "      <td>0.069980</td>\n",
       "      <td>0.015163</td>\n",
       "      <td>0.132736</td>\n",
       "      <td>0.018427</td>\n",
       "      <td>0.066703</td>\n",
       "      <td>0.012972</td>\n",
       "      <td>0.132839</td>\n",
       "      <td>0.016003</td>\n",
       "      <td>0.067646</td>\n",
       "      <td>0.013343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>0.218145</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.035503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.181479</td>\n",
       "      <td>0.432507</td>\n",
       "      <td>0.291533</td>\n",
       "      <td>0.237766</td>\n",
       "      <td>0.008559</td>\n",
       "      <td>0.015184</td>\n",
       "      <td>0.014732</td>\n",
       "      <td>0.231230</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.011228</td>\n",
       "      <td>0.008787</td>\n",
       "      <td>0.231395</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.011431</td>\n",
       "      <td>0.009196</td>\n",
       "      <td>0.229750</td>\n",
       "      <td>0.012424</td>\n",
       "      <td>0.013362</td>\n",
       "      <td>0.012818</td>\n",
       "      <td>0.226909</td>\n",
       "      <td>0.010055</td>\n",
       "      <td>0.011351</td>\n",
       "      <td>0.011169</td>\n",
       "      <td>0.229283</td>\n",
       "      <td>0.008249</td>\n",
       "      <td>0.007385</td>\n",
       "      <td>0.007981</td>\n",
       "      <td>0.232230</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.006867</td>\n",
       "      <td>0.004804</td>\n",
       "      <td>0.232070</td>\n",
       "      <td>0.010221</td>\n",
       "      <td>0.008148</td>\n",
       "      <td>0.005695</td>\n",
       "      <td>0.232827</td>\n",
       "      <td>0.011461</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>0.230222</td>\n",
       "      <td>0.009413</td>\n",
       "      <td>0.008631</td>\n",
       "      <td>0.005856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>0.969013</td>\n",
       "      <td>2.143715</td>\n",
       "      <td>0.732630</td>\n",
       "      <td>0.668701</td>\n",
       "      <td>0.223191</td>\n",
       "      <td>0.013386</td>\n",
       "      <td>0.027228</td>\n",
       "      <td>0.013557</td>\n",
       "      <td>0.220460</td>\n",
       "      <td>0.010014</td>\n",
       "      <td>0.019025</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>0.222709</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.015503</td>\n",
       "      <td>0.011726</td>\n",
       "      <td>0.225182</td>\n",
       "      <td>0.016552</td>\n",
       "      <td>0.017951</td>\n",
       "      <td>0.014456</td>\n",
       "      <td>0.225860</td>\n",
       "      <td>0.013913</td>\n",
       "      <td>0.014782</td>\n",
       "      <td>0.015533</td>\n",
       "      <td>0.230695</td>\n",
       "      <td>0.012413</td>\n",
       "      <td>0.014236</td>\n",
       "      <td>0.015546</td>\n",
       "      <td>0.226410</td>\n",
       "      <td>0.012478</td>\n",
       "      <td>0.015848</td>\n",
       "      <td>0.015096</td>\n",
       "      <td>0.224845</td>\n",
       "      <td>0.019605</td>\n",
       "      <td>0.013164</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.229622</td>\n",
       "      <td>0.015009</td>\n",
       "      <td>0.017134</td>\n",
       "      <td>0.015715</td>\n",
       "      <td>0.227262</td>\n",
       "      <td>0.011922</td>\n",
       "      <td>0.012706</td>\n",
       "      <td>0.014154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.093860</td>\n",
       "      <td>0.136025</td>\n",
       "      <td>0.969076</td>\n",
       "      <td>0.641620</td>\n",
       "      <td>0.045644</td>\n",
       "      <td>0.018626</td>\n",
       "      <td>0.092268</td>\n",
       "      <td>0.023088</td>\n",
       "      <td>0.055625</td>\n",
       "      <td>0.010684</td>\n",
       "      <td>0.076129</td>\n",
       "      <td>0.016290</td>\n",
       "      <td>0.057659</td>\n",
       "      <td>0.008596</td>\n",
       "      <td>0.075997</td>\n",
       "      <td>0.014828</td>\n",
       "      <td>0.057565</td>\n",
       "      <td>0.008885</td>\n",
       "      <td>0.075833</td>\n",
       "      <td>0.012109</td>\n",
       "      <td>0.058681</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>0.073339</td>\n",
       "      <td>0.010804</td>\n",
       "      <td>0.059502</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>0.069368</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>0.060819</td>\n",
       "      <td>0.005873</td>\n",
       "      <td>0.069790</td>\n",
       "      <td>0.012403</td>\n",
       "      <td>0.061435</td>\n",
       "      <td>0.004978</td>\n",
       "      <td>0.069732</td>\n",
       "      <td>0.011943</td>\n",
       "      <td>0.063041</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.070355</td>\n",
       "      <td>0.011480</td>\n",
       "      <td>0.063661</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.073822</td>\n",
       "      <td>0.017072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>0.080564</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.049461</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.162374</td>\n",
       "      <td>0.231994</td>\n",
       "      <td>0.760188</td>\n",
       "      <td>0.369066</td>\n",
       "      <td>0.084890</td>\n",
       "      <td>0.008301</td>\n",
       "      <td>0.031002</td>\n",
       "      <td>0.012461</td>\n",
       "      <td>0.086758</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.026615</td>\n",
       "      <td>0.008622</td>\n",
       "      <td>0.087219</td>\n",
       "      <td>0.003995</td>\n",
       "      <td>0.025938</td>\n",
       "      <td>0.007983</td>\n",
       "      <td>0.084498</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>0.028641</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.084408</td>\n",
       "      <td>0.005732</td>\n",
       "      <td>0.028824</td>\n",
       "      <td>0.011346</td>\n",
       "      <td>0.082173</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>0.030446</td>\n",
       "      <td>0.012482</td>\n",
       "      <td>0.082996</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.029621</td>\n",
       "      <td>0.014074</td>\n",
       "      <td>0.084661</td>\n",
       "      <td>0.005384</td>\n",
       "      <td>0.028257</td>\n",
       "      <td>0.014893</td>\n",
       "      <td>0.083938</td>\n",
       "      <td>0.005091</td>\n",
       "      <td>0.028382</td>\n",
       "      <td>0.014986</td>\n",
       "      <td>0.084813</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>0.028183</td>\n",
       "      <td>0.015195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>1.674513</td>\n",
       "      <td>1.563955</td>\n",
       "      <td>2.019840</td>\n",
       "      <td>1.248009</td>\n",
       "      <td>0.087096</td>\n",
       "      <td>0.007704</td>\n",
       "      <td>0.039529</td>\n",
       "      <td>0.021068</td>\n",
       "      <td>0.086721</td>\n",
       "      <td>0.006845</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.019392</td>\n",
       "      <td>0.082855</td>\n",
       "      <td>0.006181</td>\n",
       "      <td>0.025349</td>\n",
       "      <td>0.017439</td>\n",
       "      <td>0.086797</td>\n",
       "      <td>0.006191</td>\n",
       "      <td>0.020961</td>\n",
       "      <td>0.017242</td>\n",
       "      <td>0.086008</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.020600</td>\n",
       "      <td>0.017019</td>\n",
       "      <td>0.088374</td>\n",
       "      <td>0.004164</td>\n",
       "      <td>0.025063</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>0.089918</td>\n",
       "      <td>0.005303</td>\n",
       "      <td>0.022927</td>\n",
       "      <td>0.014220</td>\n",
       "      <td>0.087908</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.016578</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.087844</td>\n",
       "      <td>0.004729</td>\n",
       "      <td>0.015640</td>\n",
       "      <td>0.013639</td>\n",
       "      <td>0.089390</td>\n",
       "      <td>0.003578</td>\n",
       "      <td>0.015363</td>\n",
       "      <td>0.014528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.185266</td>\n",
       "      <td>0.182455</td>\n",
       "      <td>0.877222</td>\n",
       "      <td>0.648640</td>\n",
       "      <td>0.027906</td>\n",
       "      <td>0.018622</td>\n",
       "      <td>0.146354</td>\n",
       "      <td>0.016095</td>\n",
       "      <td>0.040356</td>\n",
       "      <td>0.011343</td>\n",
       "      <td>0.145953</td>\n",
       "      <td>0.014051</td>\n",
       "      <td>0.045037</td>\n",
       "      <td>0.011816</td>\n",
       "      <td>0.136128</td>\n",
       "      <td>0.012914</td>\n",
       "      <td>0.046020</td>\n",
       "      <td>0.012782</td>\n",
       "      <td>0.136309</td>\n",
       "      <td>0.008925</td>\n",
       "      <td>0.044869</td>\n",
       "      <td>0.012667</td>\n",
       "      <td>0.131906</td>\n",
       "      <td>0.006826</td>\n",
       "      <td>0.045460</td>\n",
       "      <td>0.009485</td>\n",
       "      <td>0.133229</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>0.044809</td>\n",
       "      <td>0.008670</td>\n",
       "      <td>0.134933</td>\n",
       "      <td>0.007555</td>\n",
       "      <td>0.043835</td>\n",
       "      <td>0.007661</td>\n",
       "      <td>0.136834</td>\n",
       "      <td>0.006587</td>\n",
       "      <td>0.044231</td>\n",
       "      <td>0.007550</td>\n",
       "      <td>0.134347</td>\n",
       "      <td>0.008742</td>\n",
       "      <td>0.045814</td>\n",
       "      <td>0.005377</td>\n",
       "      <td>0.132961</td>\n",
       "      <td>0.005367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>0.121763</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093673</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.109293</td>\n",
       "      <td>0.233427</td>\n",
       "      <td>0.477451</td>\n",
       "      <td>0.317240</td>\n",
       "      <td>0.143369</td>\n",
       "      <td>0.038874</td>\n",
       "      <td>0.068513</td>\n",
       "      <td>0.019694</td>\n",
       "      <td>0.156363</td>\n",
       "      <td>0.031738</td>\n",
       "      <td>0.061364</td>\n",
       "      <td>0.019561</td>\n",
       "      <td>0.168174</td>\n",
       "      <td>0.019735</td>\n",
       "      <td>0.039410</td>\n",
       "      <td>0.010982</td>\n",
       "      <td>0.192989</td>\n",
       "      <td>0.023830</td>\n",
       "      <td>0.046554</td>\n",
       "      <td>0.022458</td>\n",
       "      <td>0.185708</td>\n",
       "      <td>0.008031</td>\n",
       "      <td>0.038131</td>\n",
       "      <td>0.013064</td>\n",
       "      <td>0.186947</td>\n",
       "      <td>0.012538</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>0.014041</td>\n",
       "      <td>0.172122</td>\n",
       "      <td>0.024343</td>\n",
       "      <td>0.031658</td>\n",
       "      <td>0.010678</td>\n",
       "      <td>0.179883</td>\n",
       "      <td>0.018510</td>\n",
       "      <td>0.030360</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.174148</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>0.030758</td>\n",
       "      <td>0.006646</td>\n",
       "      <td>0.172274</td>\n",
       "      <td>0.026138</td>\n",
       "      <td>0.034080</td>\n",
       "      <td>0.010924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>2.083238</td>\n",
       "      <td>2.848688</td>\n",
       "      <td>1.410593</td>\n",
       "      <td>0.981430</td>\n",
       "      <td>0.137035</td>\n",
       "      <td>0.042491</td>\n",
       "      <td>0.081827</td>\n",
       "      <td>0.036483</td>\n",
       "      <td>0.147699</td>\n",
       "      <td>0.039022</td>\n",
       "      <td>0.059389</td>\n",
       "      <td>0.037991</td>\n",
       "      <td>0.154762</td>\n",
       "      <td>0.036500</td>\n",
       "      <td>0.048575</td>\n",
       "      <td>0.035621</td>\n",
       "      <td>0.151540</td>\n",
       "      <td>0.036715</td>\n",
       "      <td>0.053359</td>\n",
       "      <td>0.041589</td>\n",
       "      <td>0.183310</td>\n",
       "      <td>0.026398</td>\n",
       "      <td>0.031058</td>\n",
       "      <td>0.029800</td>\n",
       "      <td>0.181016</td>\n",
       "      <td>0.025980</td>\n",
       "      <td>0.024189</td>\n",
       "      <td>0.025743</td>\n",
       "      <td>0.180185</td>\n",
       "      <td>0.026009</td>\n",
       "      <td>0.024607</td>\n",
       "      <td>0.032151</td>\n",
       "      <td>0.183533</td>\n",
       "      <td>0.028071</td>\n",
       "      <td>0.024333</td>\n",
       "      <td>0.035813</td>\n",
       "      <td>0.179730</td>\n",
       "      <td>0.026990</td>\n",
       "      <td>0.024238</td>\n",
       "      <td>0.035916</td>\n",
       "      <td>0.174972</td>\n",
       "      <td>0.028481</td>\n",
       "      <td>0.027503</td>\n",
       "      <td>0.034668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.080046</td>\n",
       "      <td>0.076568</td>\n",
       "      <td>1.281326</td>\n",
       "      <td>0.682954</td>\n",
       "      <td>0.016469</td>\n",
       "      <td>0.006761</td>\n",
       "      <td>0.123861</td>\n",
       "      <td>0.024005</td>\n",
       "      <td>0.018798</td>\n",
       "      <td>0.005203</td>\n",
       "      <td>0.108558</td>\n",
       "      <td>0.018793</td>\n",
       "      <td>0.017972</td>\n",
       "      <td>0.005672</td>\n",
       "      <td>0.111534</td>\n",
       "      <td>0.020339</td>\n",
       "      <td>0.016973</td>\n",
       "      <td>0.005803</td>\n",
       "      <td>0.119170</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>0.016813</td>\n",
       "      <td>0.006113</td>\n",
       "      <td>0.120038</td>\n",
       "      <td>0.031563</td>\n",
       "      <td>0.015941</td>\n",
       "      <td>0.005603</td>\n",
       "      <td>0.123304</td>\n",
       "      <td>0.031813</td>\n",
       "      <td>0.015711</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.128256</td>\n",
       "      <td>0.028880</td>\n",
       "      <td>0.015468</td>\n",
       "      <td>0.004624</td>\n",
       "      <td>0.131710</td>\n",
       "      <td>0.026811</td>\n",
       "      <td>0.015182</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.132838</td>\n",
       "      <td>0.027620</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>0.003086</td>\n",
       "      <td>0.130927</td>\n",
       "      <td>0.029263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>0.035514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.064982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.147031</td>\n",
       "      <td>0.150544</td>\n",
       "      <td>0.675063</td>\n",
       "      <td>0.520678</td>\n",
       "      <td>0.035548</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.055479</td>\n",
       "      <td>0.015668</td>\n",
       "      <td>0.035630</td>\n",
       "      <td>0.005338</td>\n",
       "      <td>0.042423</td>\n",
       "      <td>0.024499</td>\n",
       "      <td>0.038266</td>\n",
       "      <td>0.007307</td>\n",
       "      <td>0.035021</td>\n",
       "      <td>0.028767</td>\n",
       "      <td>0.039784</td>\n",
       "      <td>0.006431</td>\n",
       "      <td>0.032842</td>\n",
       "      <td>0.021637</td>\n",
       "      <td>0.040072</td>\n",
       "      <td>0.006153</td>\n",
       "      <td>0.024475</td>\n",
       "      <td>0.019693</td>\n",
       "      <td>0.040530</td>\n",
       "      <td>0.006074</td>\n",
       "      <td>0.022785</td>\n",
       "      <td>0.017974</td>\n",
       "      <td>0.041975</td>\n",
       "      <td>0.005747</td>\n",
       "      <td>0.020112</td>\n",
       "      <td>0.019403</td>\n",
       "      <td>0.042832</td>\n",
       "      <td>0.005285</td>\n",
       "      <td>0.018846</td>\n",
       "      <td>0.017982</td>\n",
       "      <td>0.042661</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.021058</td>\n",
       "      <td>0.017382</td>\n",
       "      <td>0.042840</td>\n",
       "      <td>0.004109</td>\n",
       "      <td>0.018913</td>\n",
       "      <td>0.017688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>1.203161</td>\n",
       "      <td>0.999451</td>\n",
       "      <td>2.621106</td>\n",
       "      <td>1.413399</td>\n",
       "      <td>0.036402</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.061618</td>\n",
       "      <td>0.031513</td>\n",
       "      <td>0.036907</td>\n",
       "      <td>0.007028</td>\n",
       "      <td>0.064421</td>\n",
       "      <td>0.025012</td>\n",
       "      <td>0.039210</td>\n",
       "      <td>0.005159</td>\n",
       "      <td>0.044256</td>\n",
       "      <td>0.031989</td>\n",
       "      <td>0.040897</td>\n",
       "      <td>0.004768</td>\n",
       "      <td>0.038210</td>\n",
       "      <td>0.028023</td>\n",
       "      <td>0.041475</td>\n",
       "      <td>0.004208</td>\n",
       "      <td>0.034598</td>\n",
       "      <td>0.020674</td>\n",
       "      <td>0.040484</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.045030</td>\n",
       "      <td>0.025673</td>\n",
       "      <td>0.039198</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.034754</td>\n",
       "      <td>0.027557</td>\n",
       "      <td>0.039455</td>\n",
       "      <td>0.004949</td>\n",
       "      <td>0.034633</td>\n",
       "      <td>0.026416</td>\n",
       "      <td>0.040129</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>0.029612</td>\n",
       "      <td>0.022956</td>\n",
       "      <td>0.039079</td>\n",
       "      <td>0.005584</td>\n",
       "      <td>0.030786</td>\n",
       "      <td>0.025735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.063555</td>\n",
       "      <td>0.060651</td>\n",
       "      <td>0.603757</td>\n",
       "      <td>0.445644</td>\n",
       "      <td>0.137279</td>\n",
       "      <td>0.051575</td>\n",
       "      <td>0.070422</td>\n",
       "      <td>0.020450</td>\n",
       "      <td>0.202396</td>\n",
       "      <td>0.019691</td>\n",
       "      <td>0.044555</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.205876</td>\n",
       "      <td>0.011738</td>\n",
       "      <td>0.048256</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>0.209200</td>\n",
       "      <td>0.011137</td>\n",
       "      <td>0.047732</td>\n",
       "      <td>0.012088</td>\n",
       "      <td>0.212351</td>\n",
       "      <td>0.008550</td>\n",
       "      <td>0.044358</td>\n",
       "      <td>0.006447</td>\n",
       "      <td>0.213215</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.040853</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.212557</td>\n",
       "      <td>0.006963</td>\n",
       "      <td>0.040279</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.217998</td>\n",
       "      <td>0.006910</td>\n",
       "      <td>0.039680</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.214527</td>\n",
       "      <td>0.004449</td>\n",
       "      <td>0.039681</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.217721</td>\n",
       "      <td>0.003106</td>\n",
       "      <td>0.039265</td>\n",
       "      <td>0.001031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>0.262414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019341</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>2.715201</td>\n",
       "      <td>1.838007</td>\n",
       "      <td>1.405417</td>\n",
       "      <td>0.611363</td>\n",
       "      <td>0.263217</td>\n",
       "      <td>0.015160</td>\n",
       "      <td>0.014905</td>\n",
       "      <td>0.007408</td>\n",
       "      <td>0.263442</td>\n",
       "      <td>0.007030</td>\n",
       "      <td>0.011605</td>\n",
       "      <td>0.008202</td>\n",
       "      <td>0.266659</td>\n",
       "      <td>0.006216</td>\n",
       "      <td>0.009330</td>\n",
       "      <td>0.007545</td>\n",
       "      <td>0.264903</td>\n",
       "      <td>0.006868</td>\n",
       "      <td>0.007603</td>\n",
       "      <td>0.008653</td>\n",
       "      <td>0.268093</td>\n",
       "      <td>0.004772</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>0.008024</td>\n",
       "      <td>0.268721</td>\n",
       "      <td>0.003495</td>\n",
       "      <td>0.006676</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>0.267818</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.007031</td>\n",
       "      <td>0.266704</td>\n",
       "      <td>0.005710</td>\n",
       "      <td>0.007456</td>\n",
       "      <td>0.006773</td>\n",
       "      <td>0.266621</td>\n",
       "      <td>0.003157</td>\n",
       "      <td>0.005691</td>\n",
       "      <td>0.007282</td>\n",
       "      <td>0.264840</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.008196</td>\n",
       "      <td>0.007241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.199023</td>\n",
       "      <td>0.162076</td>\n",
       "      <td>0.477659</td>\n",
       "      <td>0.338850</td>\n",
       "      <td>0.257448</td>\n",
       "      <td>0.011624</td>\n",
       "      <td>0.013993</td>\n",
       "      <td>0.011186</td>\n",
       "      <td>0.262722</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.011171</td>\n",
       "      <td>0.007367</td>\n",
       "      <td>0.265824</td>\n",
       "      <td>0.006634</td>\n",
       "      <td>0.008361</td>\n",
       "      <td>0.007235</td>\n",
       "      <td>0.269054</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.007748</td>\n",
       "      <td>0.007727</td>\n",
       "      <td>0.262890</td>\n",
       "      <td>0.009174</td>\n",
       "      <td>0.010040</td>\n",
       "      <td>0.009860</td>\n",
       "      <td>0.269946</td>\n",
       "      <td>0.005563</td>\n",
       "      <td>0.009116</td>\n",
       "      <td>0.009055</td>\n",
       "      <td>0.270487</td>\n",
       "      <td>0.009170</td>\n",
       "      <td>0.009840</td>\n",
       "      <td>0.011948</td>\n",
       "      <td>0.268698</td>\n",
       "      <td>0.006633</td>\n",
       "      <td>0.011671</td>\n",
       "      <td>0.011455</td>\n",
       "      <td>0.269384</td>\n",
       "      <td>0.005913</td>\n",
       "      <td>0.008952</td>\n",
       "      <td>0.008037</td>\n",
       "      <td>0.270264</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.008752</td>\n",
       "      <td>0.008717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.348845</td>\n",
       "      <td>0.517688</td>\n",
       "      <td>0.404509</td>\n",
       "      <td>0.237023</td>\n",
       "      <td>1.099871</td>\n",
       "      <td>0.261712</td>\n",
       "      <td>0.070754</td>\n",
       "      <td>0.015655</td>\n",
       "      <td>1.605308</td>\n",
       "      <td>0.139076</td>\n",
       "      <td>0.046937</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>1.519631</td>\n",
       "      <td>0.132942</td>\n",
       "      <td>0.053206</td>\n",
       "      <td>0.009702</td>\n",
       "      <td>1.512809</td>\n",
       "      <td>0.112912</td>\n",
       "      <td>0.055617</td>\n",
       "      <td>0.009615</td>\n",
       "      <td>1.517006</td>\n",
       "      <td>0.111039</td>\n",
       "      <td>0.059200</td>\n",
       "      <td>0.007526</td>\n",
       "      <td>1.466825</td>\n",
       "      <td>0.071298</td>\n",
       "      <td>0.060451</td>\n",
       "      <td>0.009008</td>\n",
       "      <td>1.421323</td>\n",
       "      <td>0.059088</td>\n",
       "      <td>0.065671</td>\n",
       "      <td>0.008776</td>\n",
       "      <td>1.435300</td>\n",
       "      <td>0.104381</td>\n",
       "      <td>0.065192</td>\n",
       "      <td>0.010706</td>\n",
       "      <td>1.403151</td>\n",
       "      <td>0.082910</td>\n",
       "      <td>0.065861</td>\n",
       "      <td>0.008733</td>\n",
       "      <td>1.403396</td>\n",
       "      <td>0.112784</td>\n",
       "      <td>0.063882</td>\n",
       "      <td>0.009186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>2.314227</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019573</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>1.202716</td>\n",
       "      <td>1.100613</td>\n",
       "      <td>0.645340</td>\n",
       "      <td>0.302782</td>\n",
       "      <td>2.367677</td>\n",
       "      <td>0.038582</td>\n",
       "      <td>0.005855</td>\n",
       "      <td>0.002385</td>\n",
       "      <td>2.356618</td>\n",
       "      <td>0.034117</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.002339</td>\n",
       "      <td>2.357111</td>\n",
       "      <td>0.027897</td>\n",
       "      <td>0.002766</td>\n",
       "      <td>0.002219</td>\n",
       "      <td>2.360938</td>\n",
       "      <td>0.027283</td>\n",
       "      <td>0.003844</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>2.358038</td>\n",
       "      <td>0.025632</td>\n",
       "      <td>0.004103</td>\n",
       "      <td>0.004283</td>\n",
       "      <td>2.350132</td>\n",
       "      <td>0.027352</td>\n",
       "      <td>0.003534</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>2.352595</td>\n",
       "      <td>0.037431</td>\n",
       "      <td>0.004437</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>2.369297</td>\n",
       "      <td>0.083733</td>\n",
       "      <td>0.006142</td>\n",
       "      <td>0.005712</td>\n",
       "      <td>2.352228</td>\n",
       "      <td>0.041322</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>2.360263</td>\n",
       "      <td>0.019965</td>\n",
       "      <td>0.002533</td>\n",
       "      <td>0.001392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.096882</td>\n",
       "      <td>0.124257</td>\n",
       "      <td>0.463820</td>\n",
       "      <td>0.082187</td>\n",
       "      <td>2.156625</td>\n",
       "      <td>0.258066</td>\n",
       "      <td>0.020230</td>\n",
       "      <td>0.012970</td>\n",
       "      <td>2.261400</td>\n",
       "      <td>0.115872</td>\n",
       "      <td>0.014380</td>\n",
       "      <td>0.006944</td>\n",
       "      <td>2.349337</td>\n",
       "      <td>0.065461</td>\n",
       "      <td>0.007016</td>\n",
       "      <td>0.004019</td>\n",
       "      <td>2.330816</td>\n",
       "      <td>0.048262</td>\n",
       "      <td>0.004850</td>\n",
       "      <td>0.004809</td>\n",
       "      <td>2.336278</td>\n",
       "      <td>0.051447</td>\n",
       "      <td>0.004876</td>\n",
       "      <td>0.005376</td>\n",
       "      <td>2.347748</td>\n",
       "      <td>0.026503</td>\n",
       "      <td>0.003074</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>2.314609</td>\n",
       "      <td>0.101556</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.006705</td>\n",
       "      <td>2.383315</td>\n",
       "      <td>0.038294</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.003994</td>\n",
       "      <td>2.443573</td>\n",
       "      <td>0.137353</td>\n",
       "      <td>0.007764</td>\n",
       "      <td>0.007199</td>\n",
       "      <td>2.368905</td>\n",
       "      <td>0.088670</td>\n",
       "      <td>0.006182</td>\n",
       "      <td>0.005928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.119266</td>\n",
       "      <td>0.140409</td>\n",
       "      <td>0.830752</td>\n",
       "      <td>0.511955</td>\n",
       "      <td>0.019938</td>\n",
       "      <td>0.010826</td>\n",
       "      <td>0.146315</td>\n",
       "      <td>0.018515</td>\n",
       "      <td>0.030984</td>\n",
       "      <td>0.006712</td>\n",
       "      <td>0.136563</td>\n",
       "      <td>0.010285</td>\n",
       "      <td>0.031762</td>\n",
       "      <td>0.008434</td>\n",
       "      <td>0.137616</td>\n",
       "      <td>0.008665</td>\n",
       "      <td>0.032513</td>\n",
       "      <td>0.009619</td>\n",
       "      <td>0.141978</td>\n",
       "      <td>0.014316</td>\n",
       "      <td>0.031730</td>\n",
       "      <td>0.010721</td>\n",
       "      <td>0.144215</td>\n",
       "      <td>0.014539</td>\n",
       "      <td>0.030893</td>\n",
       "      <td>0.010514</td>\n",
       "      <td>0.143004</td>\n",
       "      <td>0.012610</td>\n",
       "      <td>0.030271</td>\n",
       "      <td>0.010158</td>\n",
       "      <td>0.142240</td>\n",
       "      <td>0.009185</td>\n",
       "      <td>0.028914</td>\n",
       "      <td>0.009932</td>\n",
       "      <td>0.142668</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.029137</td>\n",
       "      <td>0.009276</td>\n",
       "      <td>0.140045</td>\n",
       "      <td>0.006192</td>\n",
       "      <td>0.028351</td>\n",
       "      <td>0.008834</td>\n",
       "      <td>0.140785</td>\n",
       "      <td>0.006865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>0.080833</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.102907</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.304301</td>\n",
       "      <td>0.488774</td>\n",
       "      <td>0.555365</td>\n",
       "      <td>0.376830</td>\n",
       "      <td>0.093085</td>\n",
       "      <td>0.042806</td>\n",
       "      <td>0.081041</td>\n",
       "      <td>0.027373</td>\n",
       "      <td>0.135223</td>\n",
       "      <td>0.040950</td>\n",
       "      <td>0.061857</td>\n",
       "      <td>0.024909</td>\n",
       "      <td>0.118953</td>\n",
       "      <td>0.032240</td>\n",
       "      <td>0.057745</td>\n",
       "      <td>0.032203</td>\n",
       "      <td>0.117117</td>\n",
       "      <td>0.025424</td>\n",
       "      <td>0.045333</td>\n",
       "      <td>0.021212</td>\n",
       "      <td>0.122671</td>\n",
       "      <td>0.031834</td>\n",
       "      <td>0.046026</td>\n",
       "      <td>0.022367</td>\n",
       "      <td>0.101380</td>\n",
       "      <td>0.029599</td>\n",
       "      <td>0.056861</td>\n",
       "      <td>0.028430</td>\n",
       "      <td>0.122421</td>\n",
       "      <td>0.034230</td>\n",
       "      <td>0.044265</td>\n",
       "      <td>0.023384</td>\n",
       "      <td>0.111316</td>\n",
       "      <td>0.032047</td>\n",
       "      <td>0.047814</td>\n",
       "      <td>0.024905</td>\n",
       "      <td>0.105172</td>\n",
       "      <td>0.034145</td>\n",
       "      <td>0.054897</td>\n",
       "      <td>0.025752</td>\n",
       "      <td>0.103900</td>\n",
       "      <td>0.031866</td>\n",
       "      <td>0.051803</td>\n",
       "      <td>0.028255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>0.885552</td>\n",
       "      <td>1.151519</td>\n",
       "      <td>0.919557</td>\n",
       "      <td>0.866474</td>\n",
       "      <td>0.112806</td>\n",
       "      <td>0.029579</td>\n",
       "      <td>0.057764</td>\n",
       "      <td>0.026648</td>\n",
       "      <td>0.125433</td>\n",
       "      <td>0.023604</td>\n",
       "      <td>0.031092</td>\n",
       "      <td>0.022254</td>\n",
       "      <td>0.136938</td>\n",
       "      <td>0.020758</td>\n",
       "      <td>0.026973</td>\n",
       "      <td>0.023397</td>\n",
       "      <td>0.139672</td>\n",
       "      <td>0.010087</td>\n",
       "      <td>0.025108</td>\n",
       "      <td>0.010079</td>\n",
       "      <td>0.145410</td>\n",
       "      <td>0.009221</td>\n",
       "      <td>0.026318</td>\n",
       "      <td>0.027227</td>\n",
       "      <td>0.138942</td>\n",
       "      <td>0.010224</td>\n",
       "      <td>0.023516</td>\n",
       "      <td>0.014448</td>\n",
       "      <td>0.144649</td>\n",
       "      <td>0.007651</td>\n",
       "      <td>0.022929</td>\n",
       "      <td>0.015355</td>\n",
       "      <td>0.138069</td>\n",
       "      <td>0.014872</td>\n",
       "      <td>0.031005</td>\n",
       "      <td>0.029781</td>\n",
       "      <td>0.140214</td>\n",
       "      <td>0.013531</td>\n",
       "      <td>0.020623</td>\n",
       "      <td>0.011599</td>\n",
       "      <td>0.140997</td>\n",
       "      <td>0.010373</td>\n",
       "      <td>0.018975</td>\n",
       "      <td>0.014273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>GCN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.136188</td>\n",
       "      <td>0.215853</td>\n",
       "      <td>0.903014</td>\n",
       "      <td>0.546558</td>\n",
       "      <td>0.031236</td>\n",
       "      <td>0.014316</td>\n",
       "      <td>0.131349</td>\n",
       "      <td>0.015523</td>\n",
       "      <td>0.047867</td>\n",
       "      <td>0.011592</td>\n",
       "      <td>0.132681</td>\n",
       "      <td>0.018134</td>\n",
       "      <td>0.051409</td>\n",
       "      <td>0.012104</td>\n",
       "      <td>0.126155</td>\n",
       "      <td>0.005678</td>\n",
       "      <td>0.049425</td>\n",
       "      <td>0.013053</td>\n",
       "      <td>0.122561</td>\n",
       "      <td>0.004530</td>\n",
       "      <td>0.047561</td>\n",
       "      <td>0.012194</td>\n",
       "      <td>0.120313</td>\n",
       "      <td>0.005314</td>\n",
       "      <td>0.043520</td>\n",
       "      <td>0.011789</td>\n",
       "      <td>0.121495</td>\n",
       "      <td>0.006535</td>\n",
       "      <td>0.040977</td>\n",
       "      <td>0.009764</td>\n",
       "      <td>0.121865</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.038788</td>\n",
       "      <td>0.008526</td>\n",
       "      <td>0.121697</td>\n",
       "      <td>0.005874</td>\n",
       "      <td>0.037794</td>\n",
       "      <td>0.007653</td>\n",
       "      <td>0.119883</td>\n",
       "      <td>0.007323</td>\n",
       "      <td>0.035369</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>0.121156</td>\n",
       "      <td>0.007186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</td>\n",
       "      <td>0.101285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.092908</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkFalse__b...</td>\n",
       "      <td>0.557634</td>\n",
       "      <td>1.160246</td>\n",
       "      <td>0.649716</td>\n",
       "      <td>0.592868</td>\n",
       "      <td>0.132169</td>\n",
       "      <td>0.018121</td>\n",
       "      <td>0.059409</td>\n",
       "      <td>0.008682</td>\n",
       "      <td>0.149198</td>\n",
       "      <td>0.021895</td>\n",
       "      <td>0.060991</td>\n",
       "      <td>0.032022</td>\n",
       "      <td>0.129423</td>\n",
       "      <td>0.030126</td>\n",
       "      <td>0.050124</td>\n",
       "      <td>0.015573</td>\n",
       "      <td>0.128520</td>\n",
       "      <td>0.018260</td>\n",
       "      <td>0.046195</td>\n",
       "      <td>0.015894</td>\n",
       "      <td>0.131703</td>\n",
       "      <td>0.014704</td>\n",
       "      <td>0.038313</td>\n",
       "      <td>0.005669</td>\n",
       "      <td>0.129189</td>\n",
       "      <td>0.024565</td>\n",
       "      <td>0.044623</td>\n",
       "      <td>0.012641</td>\n",
       "      <td>0.128658</td>\n",
       "      <td>0.018694</td>\n",
       "      <td>0.040243</td>\n",
       "      <td>0.009529</td>\n",
       "      <td>0.121061</td>\n",
       "      <td>0.017618</td>\n",
       "      <td>0.046146</td>\n",
       "      <td>0.014031</td>\n",
       "      <td>0.128809</td>\n",
       "      <td>0.021465</td>\n",
       "      <td>0.038448</td>\n",
       "      <td>0.009605</td>\n",
       "      <td>0.128533</td>\n",
       "      <td>0.019636</td>\n",
       "      <td>0.040464</td>\n",
       "      <td>0.010922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>GIN</td>\n",
       "      <td>num1_hidden32_blocks3_residualFalse_jkTrue__bi...</td>\n",
       "      <td>2.037532</td>\n",
       "      <td>2.182232</td>\n",
       "      <td>1.716754</td>\n",
       "      <td>1.159833</td>\n",
       "      <td>0.122313</td>\n",
       "      <td>0.032726</td>\n",
       "      <td>0.068651</td>\n",
       "      <td>0.027221</td>\n",
       "      <td>0.126551</td>\n",
       "      <td>0.021516</td>\n",
       "      <td>0.045267</td>\n",
       "      <td>0.025998</td>\n",
       "      <td>0.149369</td>\n",
       "      <td>0.018872</td>\n",
       "      <td>0.034825</td>\n",
       "      <td>0.022749</td>\n",
       "      <td>0.142982</td>\n",
       "      <td>0.014354</td>\n",
       "      <td>0.030542</td>\n",
       "      <td>0.015624</td>\n",
       "      <td>0.140496</td>\n",
       "      <td>0.022599</td>\n",
       "      <td>0.025720</td>\n",
       "      <td>0.019062</td>\n",
       "      <td>0.147251</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.021321</td>\n",
       "      <td>0.011964</td>\n",
       "      <td>0.141142</td>\n",
       "      <td>0.021887</td>\n",
       "      <td>0.026045</td>\n",
       "      <td>0.020604</td>\n",
       "      <td>0.146926</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>0.021004</td>\n",
       "      <td>0.012608</td>\n",
       "      <td>0.146271</td>\n",
       "      <td>0.018562</td>\n",
       "      <td>0.021350</td>\n",
       "      <td>0.012576</td>\n",
       "      <td>0.141766</td>\n",
       "      <td>0.016362</td>\n",
       "      <td>0.024933</td>\n",
       "      <td>0.013928</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       model                                       model_config  \\\n",
       "                                                                  \n",
       "0        GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "1   Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "2        GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "3        GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "4        GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "5   Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "6        GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "7        GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "8        GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "9   Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "10       GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "11       GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "12       GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "13  Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "14       GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "15       GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "16       GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "17  Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "18       GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "19       GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "20       GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "21  Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "22       GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "23       GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "24       GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "25  Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "26       GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "27       GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "28       GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "29  Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "30       GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "31       GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "32       GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "33  Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "34       GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "35       GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "36       GCN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "37  Baseline    WL__hashing__d3_iOnes__hamming__sMaxdegree__knn   \n",
       "38       GIN  num1_hidden32_blocks3_residualFalse_jkFalse__b...   \n",
       "39       GIN  num1_hidden32_blocks3_residualFalse_jkTrue__bi...   \n",
       "\n",
       "                 0                                                         1  \\\n",
       "   mean_smoothness std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0         0.205909           0.341442  0.531926     0.201731        2.307958   \n",
       "1         4.169379           0.000000  0.008526     0.000000             NaN   \n",
       "2         0.161347           0.284698  0.517959     0.081921        3.704228   \n",
       "3         1.444393           1.469585  0.525974     0.286055        4.114533   \n",
       "4         0.113081           0.079308  0.446170     0.177292        1.692998   \n",
       "5         3.285040           0.000000  0.016575     0.000000             NaN   \n",
       "6         0.160057           0.275136  0.531294     0.083877        3.060610   \n",
       "7         1.936377           2.761636  0.406734     0.246671        3.317684   \n",
       "8         0.073010           0.067966  0.740877     0.402790        0.113930   \n",
       "9         0.218145           0.000000  0.035503     0.000000             NaN   \n",
       "10        0.107300           0.181479  0.432507     0.291533        0.237766   \n",
       "11        0.969013           2.143715  0.732630     0.668701        0.223191   \n",
       "12        0.093860           0.136025  0.969076     0.641620        0.045644   \n",
       "13        0.080564           0.000000  0.049461     0.000000             NaN   \n",
       "14        0.162374           0.231994  0.760188     0.369066        0.084890   \n",
       "15        1.674513           1.563955  2.019840     1.248009        0.087096   \n",
       "16        0.185266           0.182455  0.877222     0.648640        0.027906   \n",
       "17        0.121763           0.000000  0.093673     0.000000             NaN   \n",
       "18        0.109293           0.233427  0.477451     0.317240        0.143369   \n",
       "19        2.083238           2.848688  1.410593     0.981430        0.137035   \n",
       "20        0.080046           0.076568  1.281326     0.682954        0.016469   \n",
       "21        0.035514           0.000000  0.064982     0.000000             NaN   \n",
       "22        0.147031           0.150544  0.675063     0.520678        0.035548   \n",
       "23        1.203161           0.999451  2.621106     1.413399        0.036402   \n",
       "24        0.063555           0.060651  0.603757     0.445644        0.137279   \n",
       "25        0.262414           0.000000  0.019341     0.000000             NaN   \n",
       "26        2.715201           1.838007  1.405417     0.611363        0.263217   \n",
       "27        0.199023           0.162076  0.477659     0.338850        0.257448   \n",
       "28        0.348845           0.517688  0.404509     0.237023        1.099871   \n",
       "29        2.314227           0.000000  0.019573     0.000000             NaN   \n",
       "30        1.202716           1.100613  0.645340     0.302782        2.367677   \n",
       "31        0.096882           0.124257  0.463820     0.082187        2.156625   \n",
       "32        0.119266           0.140409  0.830752     0.511955        0.019938   \n",
       "33        0.080833           0.000000  0.102907     0.000000             NaN   \n",
       "34        0.304301           0.488774  0.555365     0.376830        0.093085   \n",
       "35        0.885552           1.151519  0.919557     0.866474        0.112806   \n",
       "36        0.136188           0.215853  0.903014     0.546558        0.031236   \n",
       "37        0.101285           0.000000  0.092908     0.000000             NaN   \n",
       "38        0.557634           1.160246  0.649716     0.592868        0.132169   \n",
       "39        2.037532           2.182232  1.716754     1.159833        0.122313   \n",
       "\n",
       "                                                           2  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.786470  0.051742     0.015251        3.633380   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.448261  0.031732     0.011981        3.931013   \n",
       "3            0.092545  0.008864     0.007002        4.128061   \n",
       "4            0.389816  0.065604     0.012854        2.438547   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.333263  0.028846     0.018347        3.194418   \n",
       "7            0.033607  0.005737     0.003731        3.311760   \n",
       "8            0.040076  0.083571     0.026333        0.134291   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.008559  0.015184     0.014732        0.231230   \n",
       "11           0.013386  0.027228     0.013557        0.220460   \n",
       "12           0.018626  0.092268     0.023088        0.055625   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.008301  0.031002     0.012461        0.086758   \n",
       "15           0.007704  0.039529     0.021068        0.086721   \n",
       "16           0.018622  0.146354     0.016095        0.040356   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.038874  0.068513     0.019694        0.156363   \n",
       "19           0.042491  0.081827     0.036483        0.147699   \n",
       "20           0.006761  0.123861     0.024005        0.018798   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.006800  0.055479     0.015668        0.035630   \n",
       "23           0.006636  0.061618     0.031513        0.036907   \n",
       "24           0.051575  0.070422     0.020450        0.202396   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.015160  0.014905     0.007408        0.263442   \n",
       "27           0.011624  0.013993     0.011186        0.262722   \n",
       "28           0.261712  0.070754     0.015655        1.605308   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.038582  0.005855     0.002385        2.356618   \n",
       "31           0.258066  0.020230     0.012970        2.261400   \n",
       "32           0.010826  0.146315     0.018515        0.030984   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.042806  0.081041     0.027373        0.135223   \n",
       "35           0.029579  0.057764     0.026648        0.125433   \n",
       "36           0.014316  0.131349     0.015523        0.047867   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.018121  0.059409     0.008682        0.149198   \n",
       "39           0.032726  0.068651     0.027221        0.126551   \n",
       "\n",
       "                                                           3  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.088388  0.029570     0.003545        3.691302   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.169515  0.021079     0.011209        4.059537   \n",
       "3            0.116017  0.004978     0.004651        4.164031   \n",
       "4            0.219093  0.043552     0.009089        2.372230   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.150212  0.015151     0.008145        3.260792   \n",
       "7            0.073731  0.005033     0.003572        3.231727   \n",
       "8            0.019999  0.073168     0.015674        0.132592   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.004899  0.011228     0.008787        0.231395   \n",
       "11           0.010014  0.019025     0.014386        0.222709   \n",
       "12           0.010684  0.076129     0.016290        0.057659   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.005162  0.026615     0.008622        0.087219   \n",
       "15           0.006845  0.034548     0.019392        0.082855   \n",
       "16           0.011343  0.145953     0.014051        0.045037   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.031738  0.061364     0.019561        0.168174   \n",
       "19           0.039022  0.059389     0.037991        0.154762   \n",
       "20           0.005203  0.108558     0.018793        0.017972   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.005338  0.042423     0.024499        0.038266   \n",
       "23           0.007028  0.064421     0.025012        0.039210   \n",
       "24           0.019691  0.044555     0.004454        0.205876   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.007030  0.011605     0.008202        0.266659   \n",
       "27           0.009605  0.011171     0.007367        0.265824   \n",
       "28           0.139076  0.046937     0.006680        1.519631   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.034117  0.003974     0.002339        2.357111   \n",
       "31           0.115872  0.014380     0.006944        2.349337   \n",
       "32           0.006712  0.136563     0.010285        0.031762   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.040950  0.061857     0.024909        0.118953   \n",
       "35           0.023604  0.031092     0.022254        0.136938   \n",
       "36           0.011592  0.132681     0.018134        0.051409   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.021895  0.060991     0.032022        0.129423   \n",
       "39           0.021516  0.045267     0.025998        0.149369   \n",
       "\n",
       "                                                           4  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.032049  0.027370     0.001245        3.692072   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.144355  0.008320     0.005738        4.154589   \n",
       "3            0.050895  0.002522     0.002398        4.182329   \n",
       "4            0.206250  0.045270     0.008768        2.347039   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.080157  0.008223     0.004532        3.293764   \n",
       "7            0.171514  0.006268     0.006501        3.332320   \n",
       "8            0.019993  0.065472     0.008869        0.132847   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.006945  0.011431     0.009196        0.229750   \n",
       "11           0.011001  0.015503     0.011726        0.225182   \n",
       "12           0.008596  0.075997     0.014828        0.057565   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.003995  0.025938     0.007983        0.084498   \n",
       "15           0.006181  0.025349     0.017439        0.086797   \n",
       "16           0.011816  0.136128     0.012914        0.046020   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.019735  0.039410     0.010982        0.192989   \n",
       "19           0.036500  0.048575     0.035621        0.151540   \n",
       "20           0.005672  0.111534     0.020339        0.016973   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.007307  0.035021     0.028767        0.039784   \n",
       "23           0.005159  0.044256     0.031989        0.040897   \n",
       "24           0.011738  0.048256     0.011547        0.209200   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.006216  0.009330     0.007545        0.264903   \n",
       "27           0.006634  0.008361     0.007235        0.269054   \n",
       "28           0.132942  0.053206     0.009702        1.512809   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.027897  0.002766     0.002219        2.360938   \n",
       "31           0.065461  0.007016     0.004019        2.330816   \n",
       "32           0.008434  0.137616     0.008665        0.032513   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.032240  0.057745     0.032203        0.117117   \n",
       "35           0.020758  0.026973     0.023397        0.139672   \n",
       "36           0.012104  0.126155     0.005678        0.049425   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.030126  0.050124     0.015573        0.128520   \n",
       "39           0.018872  0.034825     0.022749        0.142982   \n",
       "\n",
       "                                                           5  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.039801  0.026823     0.000620        3.724764   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.073464  0.005507     0.003681        4.172434   \n",
       "3            0.024355  0.002344     0.001521        4.175763   \n",
       "4            0.180686  0.044804     0.007723        2.285376   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.056251  0.006305     0.002912        3.333624   \n",
       "7            0.020489  0.002344     0.001196        3.330360   \n",
       "8            0.018081  0.066003     0.009502        0.133005   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.012424  0.013362     0.012818        0.226909   \n",
       "11           0.016552  0.017951     0.014456        0.225860   \n",
       "12           0.008885  0.075833     0.012109        0.058681   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.006137  0.028641     0.008420        0.084408   \n",
       "15           0.006191  0.020961     0.017242        0.086008   \n",
       "16           0.012782  0.136309     0.008925        0.044869   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.023830  0.046554     0.022458        0.185708   \n",
       "19           0.036715  0.053359     0.041589        0.183310   \n",
       "20           0.005803  0.119170     0.024206        0.016813   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.006431  0.032842     0.021637        0.040072   \n",
       "23           0.004768  0.038210     0.028023        0.041475   \n",
       "24           0.011137  0.047732     0.012088        0.212351   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.006868  0.007603     0.008653        0.268093   \n",
       "27           0.007072  0.007748     0.007727        0.262890   \n",
       "28           0.112912  0.055617     0.009615        1.517006   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.027283  0.003844     0.001841        2.358038   \n",
       "31           0.048262  0.004850     0.004809        2.336278   \n",
       "32           0.009619  0.141978     0.014316        0.031730   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.025424  0.045333     0.021212        0.122671   \n",
       "35           0.010087  0.025108     0.010079        0.145410   \n",
       "36           0.013053  0.122561     0.004530        0.047561   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.018260  0.046195     0.015894        0.131703   \n",
       "39           0.014354  0.030542     0.015624        0.140496   \n",
       "\n",
       "                                                           6  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.077982  0.026478     0.000654        3.763451   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.030223  0.003746     0.001805        4.163180   \n",
       "3            0.014542  0.001616     0.001149        4.175362   \n",
       "4            0.177515  0.046246     0.007707        2.217949   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.040564  0.003667     0.002112        3.293128   \n",
       "7            0.030402  0.002899     0.001883        3.302647   \n",
       "8            0.019803  0.067992     0.013056        0.129370   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.010055  0.011351     0.011169        0.229283   \n",
       "11           0.013913  0.014782     0.015533        0.230695   \n",
       "12           0.008237  0.073339     0.010804        0.059502   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.005732  0.028824     0.011346        0.082173   \n",
       "15           0.005848  0.020600     0.017019        0.088374   \n",
       "16           0.012667  0.131906     0.006826        0.045460   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.008031  0.038131     0.013064        0.186947   \n",
       "19           0.026398  0.031058     0.029800        0.181016   \n",
       "20           0.006113  0.120038     0.031563        0.015941   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.006153  0.024475     0.019693        0.040530   \n",
       "23           0.004208  0.034598     0.020674        0.040484   \n",
       "24           0.008550  0.044358     0.006447        0.213215   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.004772  0.008029     0.008024        0.268721   \n",
       "27           0.009174  0.010040     0.009860        0.269946   \n",
       "28           0.111039  0.059200     0.007526        1.466825   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.025632  0.004103     0.004283        2.350132   \n",
       "31           0.051447  0.004876     0.005376        2.347748   \n",
       "32           0.010721  0.144215     0.014539        0.030893   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.031834  0.046026     0.022367        0.101380   \n",
       "35           0.009221  0.026318     0.027227        0.138942   \n",
       "36           0.012194  0.120313     0.005314        0.043520   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.014704  0.038313     0.005669        0.129189   \n",
       "39           0.022599  0.025720     0.019062        0.147251   \n",
       "\n",
       "                                                           7  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.111242  0.027051     0.002219        3.729073   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.019712  0.002774     0.000978        4.162100   \n",
       "3            0.025934  0.002340     0.001987        4.187974   \n",
       "4            0.179131  0.050430     0.008644        2.253892   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.067839  0.004239     0.003494        3.314878   \n",
       "7            0.091246  0.004859     0.007965        3.330710   \n",
       "8            0.021930  0.071525     0.015650        0.130870   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.008249  0.007385     0.007981        0.232230   \n",
       "11           0.012413  0.014236     0.015546        0.226410   \n",
       "12           0.006694  0.069368     0.009569        0.060819   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.007027  0.030446     0.012482        0.082996   \n",
       "15           0.004164  0.025063     0.014131        0.089918   \n",
       "16           0.009485  0.133229     0.006261        0.044809   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.012538  0.035739     0.014041        0.172122   \n",
       "19           0.025980  0.024189     0.025743        0.180185   \n",
       "20           0.005603  0.123304     0.031813        0.015711   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.006074  0.022785     0.017974        0.041975   \n",
       "23           0.005322  0.045030     0.025673        0.039198   \n",
       "24           0.005084  0.040853     0.001691        0.212557   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.003495  0.006676     0.006881        0.267818   \n",
       "27           0.005563  0.009116     0.009055        0.270487   \n",
       "28           0.071298  0.060451     0.009008        1.421323   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.027352  0.003534     0.002783        2.352595   \n",
       "31           0.026503  0.003074     0.002376        2.314609   \n",
       "32           0.010514  0.143004     0.012610        0.030271   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.029599  0.056861     0.028430        0.122421   \n",
       "35           0.010224  0.023516     0.014448        0.144649   \n",
       "36           0.011789  0.121495     0.006535        0.040977   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.024565  0.044623     0.012641        0.128658   \n",
       "39           0.009093  0.021321     0.011964        0.141142   \n",
       "\n",
       "                                                           8  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.080766  0.026336     0.000657        3.738248   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.021542  0.002881     0.001098        4.171343   \n",
       "3            0.044935  0.002512     0.002272        4.161577   \n",
       "4            0.159414  0.050430     0.008730        2.211590   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.059871  0.004388     0.004084        3.312380   \n",
       "7            0.023717  0.001889     0.000999        3.336876   \n",
       "8            0.019559  0.069755     0.016234        0.130774   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.006468  0.006867     0.004804        0.232070   \n",
       "11           0.012478  0.015848     0.015096        0.224845   \n",
       "12           0.005873  0.069790     0.012403        0.061435   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.005292  0.029621     0.014074        0.084661   \n",
       "15           0.005303  0.022927     0.014220        0.087908   \n",
       "16           0.008670  0.134933     0.007555        0.043835   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.024343  0.031658     0.010678        0.179883   \n",
       "19           0.026009  0.024607     0.032151        0.183533   \n",
       "20           0.005266  0.128256     0.028880        0.015468   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.005747  0.020112     0.019403        0.042832   \n",
       "23           0.004810  0.034754     0.027557        0.039455   \n",
       "24           0.006963  0.040279     0.001651        0.217998   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.005292  0.007978     0.007031        0.266704   \n",
       "27           0.009170  0.009840     0.011948        0.268698   \n",
       "28           0.059088  0.065671     0.008776        1.435300   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.037431  0.004437     0.002688        2.369297   \n",
       "31           0.101556  0.004623     0.006705        2.383315   \n",
       "32           0.010158  0.142240     0.009185        0.028914   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.034230  0.044265     0.023384        0.111316   \n",
       "35           0.007651  0.022929     0.015355        0.138069   \n",
       "36           0.009764  0.121865     0.006225        0.038788   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.018694  0.040243     0.009529        0.121061   \n",
       "39           0.021887  0.026045     0.020604        0.146926   \n",
       "\n",
       "                                                           9  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.089054  0.026059     0.000693        3.731122   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.019862  0.003001     0.000853        4.217849   \n",
       "3            0.043516  0.002041     0.001230        4.171169   \n",
       "4            0.171910  0.051626     0.009626        2.217518   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.041183  0.002994     0.002430        3.350938   \n",
       "7            0.021908  0.001816     0.000780        3.279300   \n",
       "8            0.018659  0.069980     0.015163        0.132736   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.010221  0.008148     0.005695        0.232827   \n",
       "11           0.019605  0.013164     0.015028        0.229622   \n",
       "12           0.004978  0.069732     0.011943        0.063041   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.005384  0.028257     0.014893        0.083938   \n",
       "15           0.003770  0.016578     0.013699        0.087844   \n",
       "16           0.007661  0.136834     0.006587        0.044231   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.018510  0.030360     0.008064        0.174148   \n",
       "19           0.028071  0.024333     0.035813        0.179730   \n",
       "20           0.004624  0.131710     0.026811        0.015182   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.005285  0.018846     0.017982        0.042661   \n",
       "23           0.004949  0.034633     0.026416        0.040129   \n",
       "24           0.006910  0.039680     0.000905        0.214527   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.005710  0.007456     0.006773        0.266621   \n",
       "27           0.006633  0.011671     0.011455        0.269384   \n",
       "28           0.104381  0.065192     0.010706        1.403151   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.083733  0.006142     0.005712        2.352228   \n",
       "31           0.038294  0.004459     0.003994        2.443573   \n",
       "32           0.009932  0.142668     0.005727        0.029137   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.032047  0.047814     0.024905        0.105172   \n",
       "35           0.014872  0.031005     0.029781        0.140214   \n",
       "36           0.008526  0.121697     0.005874        0.037794   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.017618  0.046146     0.014031        0.128809   \n",
       "39           0.012880  0.021004     0.012608        0.146271   \n",
       "\n",
       "                                                          10  \\\n",
       "   std_dev_smoothness mean_rmse std_dev_rmse mean_smoothness   \n",
       "0            0.092032  0.026013     0.000826        3.747509   \n",
       "1                 NaN       NaN          NaN             NaN   \n",
       "2            0.069332  0.005297     0.003443        4.202400   \n",
       "3            0.026598  0.001654     0.001563        4.173870   \n",
       "4            0.164647  0.054593     0.009983        2.153550   \n",
       "5                 NaN       NaN          NaN             NaN   \n",
       "6            0.081388  0.003935     0.003232        3.301237   \n",
       "7            0.133599  0.005876     0.009684        3.308046   \n",
       "8            0.018427  0.066703     0.012972        0.132839   \n",
       "9                 NaN       NaN          NaN             NaN   \n",
       "10           0.011461  0.006695     0.004175        0.230222   \n",
       "11           0.015009  0.017134     0.015715        0.227262   \n",
       "12           0.004499  0.070355     0.011480        0.063661   \n",
       "13                NaN       NaN          NaN             NaN   \n",
       "14           0.005091  0.028382     0.014986        0.084813   \n",
       "15           0.004729  0.015640     0.013639        0.089390   \n",
       "16           0.007550  0.134347     0.008742        0.045814   \n",
       "17                NaN       NaN          NaN             NaN   \n",
       "18           0.022486  0.030758     0.006646        0.172274   \n",
       "19           0.026990  0.024238     0.035916        0.174972   \n",
       "20           0.003851  0.132838     0.027620        0.014866   \n",
       "21                NaN       NaN          NaN             NaN   \n",
       "22           0.004709  0.021058     0.017382        0.042840   \n",
       "23           0.004716  0.029612     0.022956        0.039079   \n",
       "24           0.004449  0.039681     0.000708        0.217721   \n",
       "25                NaN       NaN          NaN             NaN   \n",
       "26           0.003157  0.005691     0.007282        0.264840   \n",
       "27           0.005913  0.008952     0.008037        0.270264   \n",
       "28           0.082910  0.065861     0.008733        1.403396   \n",
       "29                NaN       NaN          NaN             NaN   \n",
       "30           0.041322  0.003732     0.002925        2.360263   \n",
       "31           0.137353  0.007764     0.007199        2.368905   \n",
       "32           0.009276  0.140045     0.006192        0.028351   \n",
       "33                NaN       NaN          NaN             NaN   \n",
       "34           0.034145  0.054897     0.025752        0.103900   \n",
       "35           0.013531  0.020623     0.011599        0.140997   \n",
       "36           0.007653  0.119883     0.007323        0.035369   \n",
       "37                NaN       NaN          NaN             NaN   \n",
       "38           0.021465  0.038448     0.009605        0.128533   \n",
       "39           0.018562  0.021350     0.012576        0.141766   \n",
       "\n",
       "                                              \n",
       "   std_dev_smoothness mean_rmse std_dev_rmse  \n",
       "0            0.068481  0.025937     0.000866  \n",
       "1                 NaN       NaN          NaN  \n",
       "2            0.075107  0.003977     0.003426  \n",
       "3            0.019045  0.001518     0.000908  \n",
       "4            0.135963  0.057452     0.010208  \n",
       "5                 NaN       NaN          NaN  \n",
       "6            0.122493  0.006421     0.005248  \n",
       "7            0.060174  0.004980     0.005459  \n",
       "8            0.016003  0.067646     0.013343  \n",
       "9                 NaN       NaN          NaN  \n",
       "10           0.009413  0.008631     0.005856  \n",
       "11           0.011922  0.012706     0.014154  \n",
       "12           0.003705  0.073822     0.017072  \n",
       "13                NaN       NaN          NaN  \n",
       "14           0.005543  0.028183     0.015195  \n",
       "15           0.003578  0.015363     0.014528  \n",
       "16           0.005377  0.132961     0.005367  \n",
       "17                NaN       NaN          NaN  \n",
       "18           0.026138  0.034080     0.010924  \n",
       "19           0.028481  0.027503     0.034668  \n",
       "20           0.003086  0.130927     0.029263  \n",
       "21                NaN       NaN          NaN  \n",
       "22           0.004109  0.018913     0.017688  \n",
       "23           0.005584  0.030786     0.025735  \n",
       "24           0.003106  0.039265     0.001031  \n",
       "25                NaN       NaN          NaN  \n",
       "26           0.004725  0.008196     0.007241  \n",
       "27           0.006945  0.008752     0.008717  \n",
       "28           0.112784  0.063882     0.009186  \n",
       "29                NaN       NaN          NaN  \n",
       "30           0.019965  0.002533     0.001392  \n",
       "31           0.088670  0.006182     0.005928  \n",
       "32           0.008834  0.140785     0.006865  \n",
       "33                NaN       NaN          NaN  \n",
       "34           0.031866  0.051803     0.028255  \n",
       "35           0.010373  0.018975     0.014273  \n",
       "36           0.007053  0.121156     0.007186  \n",
       "37                NaN       NaN          NaN  \n",
       "38           0.019636  0.040464     0.010922  \n",
       "39           0.016362  0.024933     0.013928  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gather all results for a given fixed teacher parameterization and a fixed distance matrix for computing the smoothness\n",
    "smoothness_stats = pd.DataFrame()\n",
    "for teacher_outputs_filename in tqdm(teacher_outputs_filenames):\n",
    "    for student in students:\n",
    "        student_outputs_filename = \\\n",
    "            f\"{teacher_outputs_prefix}/{teacher_outputs_filename}/student_outputs/{student}/smoothness/{DIST_MATRIX}.csv\"\n",
    "        smoothness_stats = smoothness_stats.append(\n",
    "            readSmoothnessCSV(student_outputs_filename, model=student), ignore_index=True)\n",
    "smoothness_stats = smoothness_stats[ordered_cols]\n",
    "smoothness_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average the results throughout the different teacher initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/berges/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py:3889: PerformanceWarning: dropping on a non-lexsorted multi-index without a level parameter may impact performance.\n",
      "  obj = obj._drop_axis(labels, axis, level=level, errors=errors)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">0</th>\n",
       "      <th colspan=\"4\" halign=\"left\">1</th>\n",
       "      <th colspan=\"4\" halign=\"left\">2</th>\n",
       "      <th colspan=\"4\" halign=\"left\">3</th>\n",
       "      <th colspan=\"4\" halign=\"left\">4</th>\n",
       "      <th colspan=\"4\" halign=\"left\">5</th>\n",
       "      <th colspan=\"4\" halign=\"left\">6</th>\n",
       "      <th colspan=\"4\" halign=\"left\">7</th>\n",
       "      <th colspan=\"4\" halign=\"left\">8</th>\n",
       "      <th colspan=\"4\" halign=\"left\">9</th>\n",
       "      <th colspan=\"4\" halign=\"left\">10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "      <th>mean_smoothness</th>\n",
       "      <th>std_dev_smoothness</th>\n",
       "      <th>mean_rmse</th>\n",
       "      <th>std_dev_rmse</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>model_config</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <th>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</th>\n",
       "      <td>1.066916</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <th>num1_hidden32_blocks3_residualFalse_jkFalse__bias0_lower-0.1_upper0.1</th>\n",
       "      <td>0.141903</td>\n",
       "      <td>0.181836</td>\n",
       "      <td>0.758863</td>\n",
       "      <td>0.449621</td>\n",
       "      <td>0.549323</td>\n",
       "      <td>0.159880</td>\n",
       "      <td>0.098224</td>\n",
       "      <td>0.018777</td>\n",
       "      <td>0.820755</td>\n",
       "      <td>0.053178</td>\n",
       "      <td>0.083767</td>\n",
       "      <td>0.011700</td>\n",
       "      <td>0.812547</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.010256</td>\n",
       "      <td>0.809646</td>\n",
       "      <td>0.041276</td>\n",
       "      <td>0.083683</td>\n",
       "      <td>0.010363</td>\n",
       "      <td>0.807216</td>\n",
       "      <td>0.044482</td>\n",
       "      <td>0.083408</td>\n",
       "      <td>0.010444</td>\n",
       "      <td>0.798613</td>\n",
       "      <td>0.043277</td>\n",
       "      <td>0.084071</td>\n",
       "      <td>0.010400</td>\n",
       "      <td>0.794030</td>\n",
       "      <td>0.036552</td>\n",
       "      <td>0.084955</td>\n",
       "      <td>0.010029</td>\n",
       "      <td>0.792235</td>\n",
       "      <td>0.042664</td>\n",
       "      <td>0.085518</td>\n",
       "      <td>0.009404</td>\n",
       "      <td>0.788844</td>\n",
       "      <td>0.039530</td>\n",
       "      <td>0.085032</td>\n",
       "      <td>0.009458</td>\n",
       "      <td>0.784308</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.085383</td>\n",
       "      <td>0.010039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GIN</th>\n",
       "      <th>num1_hidden32_blocks3_residualFalse_jkFalse__bias0_lower-0.1_upper0.1</th>\n",
       "      <td>0.200524</td>\n",
       "      <td>0.329263</td>\n",
       "      <td>0.554102</td>\n",
       "      <td>0.305505</td>\n",
       "      <td>0.990574</td>\n",
       "      <td>0.117468</td>\n",
       "      <td>0.040543</td>\n",
       "      <td>0.015309</td>\n",
       "      <td>1.044395</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.032626</td>\n",
       "      <td>0.015207</td>\n",
       "      <td>1.070892</td>\n",
       "      <td>0.039695</td>\n",
       "      <td>0.025159</td>\n",
       "      <td>0.012623</td>\n",
       "      <td>1.084088</td>\n",
       "      <td>0.027755</td>\n",
       "      <td>0.023734</td>\n",
       "      <td>0.012157</td>\n",
       "      <td>1.089670</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>0.020945</td>\n",
       "      <td>0.010246</td>\n",
       "      <td>1.084350</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.021704</td>\n",
       "      <td>0.010945</td>\n",
       "      <td>1.084247</td>\n",
       "      <td>0.028691</td>\n",
       "      <td>0.019450</td>\n",
       "      <td>0.010571</td>\n",
       "      <td>1.090756</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.020170</td>\n",
       "      <td>0.010430</td>\n",
       "      <td>1.104930</td>\n",
       "      <td>0.039334</td>\n",
       "      <td>0.020619</td>\n",
       "      <td>0.010046</td>\n",
       "      <td>1.090539</td>\n",
       "      <td>0.038992</td>\n",
       "      <td>0.020741</td>\n",
       "      <td>0.011216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1</th>\n",
       "      <td>1.615170</td>\n",
       "      <td>1.805940</td>\n",
       "      <td>1.240394</td>\n",
       "      <td>0.778472</td>\n",
       "      <td>1.078195</td>\n",
       "      <td>0.031242</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>0.017701</td>\n",
       "      <td>1.080365</td>\n",
       "      <td>0.033893</td>\n",
       "      <td>0.027933</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>1.080537</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>0.021637</td>\n",
       "      <td>0.016158</td>\n",
       "      <td>1.092756</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.020226</td>\n",
       "      <td>0.014022</td>\n",
       "      <td>1.095481</td>\n",
       "      <td>0.015753</td>\n",
       "      <td>0.016972</td>\n",
       "      <td>0.014465</td>\n",
       "      <td>1.092362</td>\n",
       "      <td>0.021522</td>\n",
       "      <td>0.017076</td>\n",
       "      <td>0.012712</td>\n",
       "      <td>1.096060</td>\n",
       "      <td>0.018951</td>\n",
       "      <td>0.016393</td>\n",
       "      <td>0.013797</td>\n",
       "      <td>1.095519</td>\n",
       "      <td>0.023901</td>\n",
       "      <td>0.015817</td>\n",
       "      <td>0.014784</td>\n",
       "      <td>1.089313</td>\n",
       "      <td>0.028821</td>\n",
       "      <td>0.014555</td>\n",
       "      <td>0.013385</td>\n",
       "      <td>1.092049</td>\n",
       "      <td>0.018021</td>\n",
       "      <td>0.014749</td>\n",
       "      <td>0.013228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                          0  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn           1.066916   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.141903   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.200524   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.615170   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn              0.000000   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.181836   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.329263   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           1.805940   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn     0.050345   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.758863   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.554102   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  1.240394   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn        0.000000   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.449621   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.305505   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.778472   \n",
       "\n",
       "                                                                          1  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.549323   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.990574   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.078195   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.159880   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.117468   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.031242   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.098224   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.040543   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.037198   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.018777   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.015309   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.017701   \n",
       "\n",
       "                                                                          2  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.820755   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.044395   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.080365   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.053178   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.055519   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.033893   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.083767   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.032626   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.027933   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.011700   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.015207   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.016380   \n",
       "\n",
       "                                                                          3  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.812547   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.070892   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.080537   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.044959   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.039695   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.035499   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.082700   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.025159   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.021637   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010256   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.012623   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.016158   \n",
       "\n",
       "                                                                          4  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.809646   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.084088   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.092756   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.041276   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.027755   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.016766   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.083683   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.023734   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.020226   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010363   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.012157   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.014022   \n",
       "\n",
       "                                                                          5  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.807216   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.089670   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.095481   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.044482   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.020792   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.015753   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.083408   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.020945   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.016972   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010444   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010246   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.014465   \n",
       "\n",
       "                                                                          6  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.798613   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.084350   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.092362   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.043277   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.020767   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.021522   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.084071   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.021704   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.017076   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010400   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010945   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.012712   \n",
       "\n",
       "                                                                          7  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.794030   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.084247   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.096060   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.036552   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.028691   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.018951   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.084955   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.019450   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.016393   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010029   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010571   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.013797   \n",
       "\n",
       "                                                                          8  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.792235   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.090756   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.095519   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.042664   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.019504   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.023901   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.085518   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.020170   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.015817   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.009404   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010430   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.014784   \n",
       "\n",
       "                                                                          9  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.788844   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.104930   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.089313   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.039530   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.039334   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.028821   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.085032   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.020619   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.014555   \n",
       "\n",
       "                                                                          \\\n",
       "                                                            std_dev_rmse   \n",
       "model    model_config                                                      \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.009458   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010046   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.013385   \n",
       "\n",
       "                                                                         10  \\\n",
       "                                                            mean_smoothness   \n",
       "model    model_config                                                         \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        0.784308   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...        1.090539   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...        1.092049   \n",
       "\n",
       "                                                                                \\\n",
       "                                                            std_dev_smoothness   \n",
       "model    model_config                                                            \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn                   NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.036439   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...           0.038992   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...           0.018021   \n",
       "\n",
       "                                                                       \\\n",
       "                                                            mean_rmse   \n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.085383   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.020741   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.014749   \n",
       "\n",
       "                                                                          \n",
       "                                                            std_dev_rmse  \n",
       "model    model_config                                                     \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn             NaN  \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.010039  \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...     0.011216  \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...     0.013228  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#warnings.filterwarnings('ignore', category='PerformanceWarning')\n",
    "\n",
    "smoothness_stats_avg = smoothness_stats.groupby(['model', 'model_config']).mean()\n",
    "smoothness_stats_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "---\n",
    "Visualize the averaged obtained results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>model_config</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Baseline</th>\n",
       "      <th>WL__hashing__d3_iOnes__hamming__sMaxdegree__knn</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GCN</th>\n",
       "      <th>num1_hidden32_blocks3_residualFalse_jkFalse__bias0_lower-0.1_upper0.1</th>\n",
       "      <td>0.181836</td>\n",
       "      <td>0.159880</td>\n",
       "      <td>0.053178</td>\n",
       "      <td>0.044959</td>\n",
       "      <td>0.041276</td>\n",
       "      <td>0.044482</td>\n",
       "      <td>0.043277</td>\n",
       "      <td>0.036552</td>\n",
       "      <td>0.042664</td>\n",
       "      <td>0.039530</td>\n",
       "      <td>0.036439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GIN</th>\n",
       "      <th>num1_hidden32_blocks3_residualFalse_jkFalse__bias0_lower-0.1_upper0.1</th>\n",
       "      <td>0.329263</td>\n",
       "      <td>0.117468</td>\n",
       "      <td>0.055519</td>\n",
       "      <td>0.039695</td>\n",
       "      <td>0.027755</td>\n",
       "      <td>0.020792</td>\n",
       "      <td>0.020767</td>\n",
       "      <td>0.028691</td>\n",
       "      <td>0.019504</td>\n",
       "      <td>0.039334</td>\n",
       "      <td>0.038992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1</th>\n",
       "      <td>1.805940</td>\n",
       "      <td>0.031242</td>\n",
       "      <td>0.033893</td>\n",
       "      <td>0.035499</td>\n",
       "      <td>0.016766</td>\n",
       "      <td>0.015753</td>\n",
       "      <td>0.021522</td>\n",
       "      <td>0.018951</td>\n",
       "      <td>0.023901</td>\n",
       "      <td>0.028821</td>\n",
       "      <td>0.018021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    0  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn     0.000000   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.181836   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.329263   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  1.805940   \n",
       "\n",
       "                                                                    1  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.159880   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.117468   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.031242   \n",
       "\n",
       "                                                                    2  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.053178   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.055519   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.033893   \n",
       "\n",
       "                                                                    3  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.044959   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.039695   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.035499   \n",
       "\n",
       "                                                                    4  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.041276   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.027755   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.016766   \n",
       "\n",
       "                                                                    5  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.044482   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.020792   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.015753   \n",
       "\n",
       "                                                                    6  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.043277   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.020767   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.021522   \n",
       "\n",
       "                                                                    7  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.036552   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.028691   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.018951   \n",
       "\n",
       "                                                                    8  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.042664   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.019504   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.023901   \n",
       "\n",
       "                                                                    9  \\\n",
       "model    model_config                                                   \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN   \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.039530   \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.039334   \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.028821   \n",
       "\n",
       "                                                                   10  \n",
       "model    model_config                                                  \n",
       "Baseline WL__hashing__d3_iOnes__hamming__sMaxdegree__knn          NaN  \n",
       "GCN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.036439  \n",
       "GIN      num1_hidden32_blocks3_residualFalse_jkFalse__bi...  0.038992  \n",
       "         num1_hidden32_blocks3_residualFalse_jkTrue__bia...  0.018021  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = np.max(list(map(int, smoothness_stats_avg.columns.get_level_values(0))))\n",
    "\n",
    "epochs = list(range(num_epochs + 1))\n",
    "\n",
    "# Retrieve mean smoothness\n",
    "mean_smoothness = smoothness_stats_avg.iloc[:, smoothness_stats_avg.columns.get_level_values(1) == 'mean_smoothness']\n",
    "mean_smoothness.columns = mean_smoothness.columns.droplevel(1)\n",
    "# Retrieve std dev smoothness\n",
    "std_dev_smoothness = smoothness_stats_avg.iloc[:, smoothness_stats_avg.columns.get_level_values(1) == 'std_dev_smoothness']\n",
    "std_dev_smoothness.columns = std_dev_smoothness.columns.droplevel(1)\n",
    "# Retrieve mean RMSE\n",
    "mean_rmse = smoothness_stats_avg.iloc[:, smoothness_stats_avg.columns.get_level_values(1) == 'mean_rmse']\n",
    "mean_rmse.columns = mean_rmse.columns.droplevel(1)\n",
    "# Retrieve std dev smoothness\n",
    "std_dev_rmse = smoothness_stats_avg.iloc[:, smoothness_stats_avg.columns.get_level_values(1) == 'std_dev_rmse']\n",
    "std_dev_rmse.columns = std_dev_rmse.columns.droplevel(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_config = ('GIN', 'num1_hidden32_blocks3_residualFalse_jkFalse__bias0_lower-0.1_upper0.1')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "\n",
    "x = epochs.copy()\n",
    "y = mean_smoothness.loc[model_config].to_numpy()\n",
    "std_dev = std_dev_rmse.loc[model_config].to_numpy()\n",
    "y2 = mean_rmse.loc[model_config].to_numpy()\n",
    "std_dev2 = std_dev_rmse.loc[model_config].to_numpy()\n",
    "\n",
    "ax.plot(x, y, label=f\"{model_config[0]} ({model_config[1]})\")\n",
    "ax.fill_between(x, (y - std_dev), (y + std_dev), alpha=0.1)\n",
    "\n",
    "# Add twin performance RMSE\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(x, y2, label=f\"{model_config[0]} ({model_config[1]})\")\n",
    "ax2.fill_between(x, (y2 - std_dev2), (y2 + std_dev2), alpha=0.1)\n",
    "\n",
    "ax.set_xlim(left=epochs[0], right=epochs[-1])\n",
    "ax.set_xticks(epochs)\n",
    "#ax.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.20052419493972035,\n",
       " 0.990573815421927,\n",
       " 1.0443954078195368,\n",
       " 1.0708917760119667,\n",
       " 1.0840880575181286,\n",
       " 1.0896696273491078,\n",
       " 1.0843503943888717,\n",
       " 1.08424743282486,\n",
       " 1.0907558806811621,\n",
       " 1.104930024073393,\n",
       " 1.0905388969037122]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_smoothness.loc[('GIN', 'num1_hidden32_blocks3_residualFalse_jkFalse__bias0_lower-0.1_upper0.1')].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Baseline', 'WL__hashing__d3_iOnes__hamming__sMaxdegree__knn'),\n",
       " ('GCN',\n",
       "  'num1_hidden32_blocks3_residualFalse_jkFalse__bias0_lower-0.1_upper0.1'),\n",
       " ('GIN',\n",
       "  'num1_hidden32_blocks3_residualFalse_jkFalse__bias0_lower-0.1_upper0.1'),\n",
       " ('GIN',\n",
       "  'num1_hidden32_blocks3_residualFalse_jkTrue__bias0_lower-0.1_upper0.1')]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# way to get all index combinations\n",
    "mean_smoothness.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
